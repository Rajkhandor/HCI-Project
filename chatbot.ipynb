{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/chatbot/__results__.html\n",
      "/kaggle/input/chatbot/custom.css\n",
      "/kaggle/input/chatbot/checkpoint1.pt\n",
      "/kaggle/input/chatbot/__notebook__.ipynb\n",
      "/kaggle/input/chatbot/__output__.json\n",
      "/kaggle/input/chatterbotenglish/politics.yml\n",
      "/kaggle/input/chatterbotenglish/gossip.yml\n",
      "/kaggle/input/chatterbotenglish/emotion.yml\n",
      "/kaggle/input/chatterbotenglish/literature.yml\n",
      "/kaggle/input/chatterbotenglish/science.yml\n",
      "/kaggle/input/chatterbotenglish/movies.yml\n",
      "/kaggle/input/chatterbotenglish/psychology.yml\n",
      "/kaggle/input/chatterbotenglish/humor.yml\n",
      "/kaggle/input/chatterbotenglish/health.yml\n",
      "/kaggle/input/chatterbotenglish/money.yml\n",
      "/kaggle/input/chatterbotenglish/trivia.yml\n",
      "/kaggle/input/chatterbotenglish/food.yml\n",
      "/kaggle/input/chatterbotenglish/sports.yml\n",
      "/kaggle/input/chatterbotenglish/greetings.yml\n",
      "/kaggle/input/chatterbotenglish/computers.yml\n",
      "/kaggle/input/chatterbotenglish/ai.yml\n",
      "/kaggle/input/chatterbotenglish/botprofile.yml\n",
      "/kaggle/input/chatterbotenglish/history.yml\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        \n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>have you read the communist</td>\n",
       "      <td>yes, marx had made some interesting observations.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what is a government</td>\n",
       "      <td>ideally it is a representative of the people.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what is greenpeace</td>\n",
       "      <td>global organization promoting enviornmental ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what is capitalism</td>\n",
       "      <td>the economic system in which all or most of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is socialism</td>\n",
       "      <td>communism from people who want to keep their v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>what kind of history</td>\n",
       "      <td>I like to talk about the history of robots and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>are you interested in history</td>\n",
       "      <td>I am very interested in history, too. what per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>explain history</td>\n",
       "      <td>history has two broad interpretations, dependi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>who invented the lightbulb</td>\n",
       "      <td>thomas edison.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>who invented the steam engine</td>\n",
       "      <td>james watt.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>721 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          question  \\\n",
       "0      have you read the communist   \n",
       "1             what is a government   \n",
       "2               what is greenpeace   \n",
       "3               what is capitalism   \n",
       "4                what is socialism   \n",
       "..                             ...   \n",
       "716           what kind of history   \n",
       "717  are you interested in history   \n",
       "718                explain history   \n",
       "719     who invented the lightbulb   \n",
       "720  who invented the steam engine   \n",
       "\n",
       "                                                answer  \n",
       "0    yes, marx had made some interesting observations.  \n",
       "1        ideally it is a representative of the people.  \n",
       "2    global organization promoting enviornmental ac...  \n",
       "3    the economic system in which all or most of th...  \n",
       "4    communism from people who want to keep their v...  \n",
       "..                                                 ...  \n",
       "716  I like to talk about the history of robots and...  \n",
       "717  I am very interested in history, too. what per...  \n",
       "718  history has two broad interpretations, dependi...  \n",
       "719                                     thomas edison.  \n",
       "720                                        james watt.  \n",
       "\n",
       "[721 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "df = pd.DataFrame(columns = ['question', 'answer']) \n",
    "idx = 0\n",
    "def add_to_df(file_name):\n",
    "    global idx\n",
    "    with open(r'/kaggle/input/chatterbotenglish/' + file_name) as file:\n",
    "        content_list = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        conversation = content_list['conversations']\n",
    "        #print(content_list['conversations'][0])\n",
    "        for i in range(len(conversation)):\n",
    "            q = conversation[i][0]\n",
    "            for j in range(1,len(conversation[i])):\n",
    "                a = conversation[i][j]   \n",
    "                df.loc[idx] = [q,a]\n",
    "                idx+=1\n",
    "        #print(content_list['conversations'])\n",
    "\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        if (filename[-3:]==\"yml\"):\n",
    "            if (filename != \"literature.yml\"):\n",
    "                add_to_df(str(filename))\n",
    "\n",
    "#add_to_df('emotion.yml')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "class ChatbotDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, maxlen):\n",
    "\n",
    "        #Store the contents of the file in a pandas dataframe\n",
    "        self.df = df\n",
    "\n",
    "        #Initialize the BERT tokenizer\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.maxlen = maxlen\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        #print (index)\n",
    "        #Selecting the question and answer at the specified index in the data frame\n",
    "        question = self.df.loc[index, 'question']\n",
    "        answer = self.df.loc[index, 'answer']\n",
    "        #print (question)\n",
    "        #print (answer)\n",
    "        #Preprocessing the text to be suitable for BERT\n",
    "        tokens = self.tokenizer.tokenize(question) #Tokenize the sentence\n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]'] #Insering the CLS and SEP token in the beginning and end of the sentence\n",
    "        if len(tokens) < self.maxlen:\n",
    "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] #Padding sentences\n",
    "        else:\n",
    "            tokens = tokens[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n",
    "        #print (tokens)\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
    "        #print (tokens_ids)\n",
    "        tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor\n",
    "\n",
    "        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
    "        attn_mask = (tokens_ids_tensor != 0).long()\n",
    "        \n",
    "        ans_tokens = self.tokenizer.tokenize(answer)\n",
    "        #print (ans_tokens)\n",
    "        ans_tokens = ['[CLS]'] + ans_tokens + ['[SEP]'] #Insering the CLS and SEP token in the beginning and end of the sentence\n",
    "        ans_len = []\n",
    "        if len(ans_tokens) < self.maxlen:\n",
    "            ans_tokens = ans_tokens + ['[PAD]' for _ in range(self.maxlen - len(ans_tokens))] #Padding sentences\n",
    "            ans_len = [len(ans_tokens)]\n",
    "        else:\n",
    "            ans_tokens = ans_tokens[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n",
    "            ans_len = [len(ans_tokens)]\n",
    "        ans_tokens_ids = self.tokenizer.convert_tokens_to_ids(ans_tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
    "        #print (tokens_ids)\n",
    "        ans_tokens_ids_tensor = torch.tensor(ans_tokens_ids) #Converting the list to a pytorch tensor\n",
    "        \n",
    "        ans_len = torch.LongTensor(ans_len)\n",
    "        #print (self.tokenizer.vocab_size)\n",
    "        return tokens_ids_tensor, attn_mask, ans_tokens_ids_tensor,ans_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d824e669ae4c18b7737dd9357c86f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Creating instances of training set\n",
    "train_set = ChatbotDataset(df, maxlen = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encode_dim, freeze_bert = True):\n",
    "        super(Encoder, self).__init__()\n",
    "        #Instantiating BERT model object \n",
    "        self.bert_layer = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.encode_dim = encode_dim\n",
    "        #Freeze bert layers\n",
    "        if freeze_bert:\n",
    "            for p in self.bert_layer.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        self.fc = nn.Linear(768,encode_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, seq, attn_masks):\n",
    "        '''\n",
    "        Inputs:\n",
    "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
    "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
    "        '''\n",
    "\n",
    "        #Feeding the input to BERT model to obtain contextualized representations\n",
    "        cont_reps, _ = self.bert_layer(seq, attention_mask = attn_masks)\n",
    "        #print (cont_reps.size())  (batch_size , max_len, 768) \n",
    "        self.out = self.fc(cont_reps) # (batch_size, max_len, encode_dim)\n",
    "        #print (self.out.size())\n",
    "        return self.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c4846edd244de19a8ab6396abf2612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=361.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78f838819def4bf8b3b7e53bba86129e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = Encoder(512,freeze_bert = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_dim, decoder_dim):\n",
    "        \"\"\"\n",
    "        :param encoder_dim: feature size of encoded images = 2048\n",
    "        :param decoder_dim: size of decoder's RNN \n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, decoder_dim)  # linear layer to transform encoded vector\n",
    "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param encoder_out: encoded vector, a tensor of dimension (batch_size, max_len, encoder_dim)\n",
    "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
    "        :return: attention weighted encoding, weights\n",
    "        \"\"\"\n",
    "        #print (encoder_out.size())\n",
    "        att1 = self.encoder_att(encoder_out)  # (batch_size, max_len, decoder_dim)\n",
    "        decoder_hidden_copy=decoder_hidden.unsqueeze(2)  #(batch_size, decoder_dim,1)\n",
    "        #print (encoder_out.size())\n",
    "        #print (decoder_hidden_copy.size())\n",
    "        score = torch.bmm(att1,decoder_hidden_copy)  # (batch_size , max_len ,1)\n",
    "        \n",
    "        score = score.squeeze(2) # (batch_size , max_len)\n",
    "        \n",
    "        alpha = self.softmax(score)  # (batch_size, max_len)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
    "\n",
    "        return attention_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, decoder_dim, vocab_size, encoder_dim=256, dropout=0.5):\n",
    "        \"\"\"\n",
    "        :param embed_dim: embedding size\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param vocab_size: size of vocabulary\n",
    "        :param encoder_dim: feature size of encoded vector\n",
    "        :param dropout: dropout\n",
    "        \"\"\"\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attention = Attention(encoder_dim, decoder_dim)  # attention network\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
    "        self.init_weights()  \n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        \"\"\"\n",
    "        Creates the initial hidden and cell states for the decoder's LSTM.\n",
    "        :param encoder_out: encoded vector, a tensor of dimension (batch_size, max_len, encoder_dim)\n",
    "        :return: hidden state, cell state\n",
    "        \"\"\"\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param encoder_out: encoded vector, a tensor of dimension (batch_size, max_len, encoder_dim)\n",
    "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
    "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
    "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        batch_size = encoder_out.size(0)\n",
    "        vocab_size = self.vocab_size\n",
    "\n",
    "        max_len = encoder_out.size(1)\n",
    "        #caption_lengths = caption_lengths.squeeze(1)\n",
    "        #print (caption_lengths.size())\n",
    "        #print (encoder_out.size())\n",
    "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
    "        #print (encoder_out.size())\n",
    "        #print (sort_ind)\n",
    "        \n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        #print (encoder_out.size())\n",
    "        encoded_captions = encoded_captions.squeeze(1)\n",
    "        #print (encoded_captions)\n",
    "        #print (encoded_captions.size())\n",
    "        \n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "        #print (encoded_captions)\n",
    "        #print (encoded_captions.size())\n",
    "        # Embedding\n",
    "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
    "        #print (embeddings.size())\n",
    "        # Initialize LSTM state\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "        #print (caption_lengths)\n",
    "        #print (decode_lengths)\n",
    "        '''\n",
    "        sample_decode_lengths=[]\n",
    "        for i in decode_lengths:\n",
    "            sample_decode_lengths.append(max(i))\n",
    "        #print (sample_decode_lengths)\n",
    "        decode_lengths = sample_decode_lengths\n",
    "        '''\n",
    "        # Create tensors to hold word predicion scores and alphas\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), max_len).to(device)\n",
    "\n",
    "        # At each time-step, decode by\n",
    "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
    "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
    "        \n",
    "        #print (max(decode_lengths))\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            \n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
    "                                                                h[:batch_size_t])\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            #print (\"BEFORE GATE\")\n",
    "            #print (attention_weighted_encoding.size())\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            #print (\"AFTER GATE\")\n",
    "            #print (embeddings.size())\n",
    "            #print (attention_weighted_encoding.size())\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
    "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "\n",
    "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 15])\n",
      "torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_set, batch_size = 16, num_workers = 5)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for it, (seq, attn_masks, ans, ans_len) in enumerate(train_loader):\n",
    "        logits = net(seq, attn_masks)\n",
    "        print (ans.size())\n",
    "        print (ans_len.size())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "# Model parameters\n",
    "emb_dim = 256  # dimension of word embeddings\n",
    "decoder_dim = 256  # dimension of decoder RNN\n",
    "dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)\n",
    "# Training parameters\n",
    "start_epoch = 0\n",
    "epochs = 10  \n",
    "epochs_since_improvement = 0  \n",
    "batch_size = 16\n",
    "workers = 1  \n",
    "decoder_lr = 4e-2   \n",
    "alpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\n",
    "best_bleu4 = 0.  # BLEU-4 score right now\n",
    "print_freq = 100  \n",
    "checkpoint = '../input/chatbot/checkpoint1.pt'  #None # path to checkpoint, None if none\n",
    "data_name='1'\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#rev_word_map = {v: k for k, v in word_map.items()} # ix2word\n",
    "def save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer):\n",
    "    state = {'epoch': epoch,\n",
    "             'epochs_since_improvement': epochs_since_improvement,\n",
    "             'encoder_state_dict': encoder.state_dict(),\n",
    "             'decoder_state_dict': decoder.state_dict(),\n",
    "             'decoder_optimizer_dict': decoder_optimizer.state_dict(),\n",
    "             'encoder_optimizer_dict': encoder_optimizer.state_dict()\n",
    "            }\n",
    "    filename = 'checkpoint1.pt'\n",
    "    print (filename)\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def adjust_learning_rate(optimizer, shrink_factor):\n",
    "    \"\"\"\n",
    "    Shrinks learning rate by a specified factor.\n",
    "    :param optimizer: optimizer whose learning rate must be shrunk.\n",
    "    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n",
    "    \"\"\"\n",
    "    print(\"\\nDECAYING learning rate.\")\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
    "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))    \n",
    "    \n",
    "\n",
    "def train(train_loader, encoder, decoder, criterion, encoder_optimizer,decoder_optimizer, epoch , start ):\n",
    "    decoder.train()  \n",
    "    encoder.train()\n",
    "    \n",
    "    for i, (seq, attn_masks, ans, ans_len) in enumerate(train_loader):\n",
    "        current_time = time.time()\n",
    "        if (current_time-start >= 6*3600):\n",
    "            break\n",
    "        \n",
    "        # Move to GPU, if available\n",
    "        seq = seq.to(device)\n",
    "        attn_masks = attn_masks.to(device)\n",
    "        ans = ans.to(device)\n",
    "        ans_len = ans_len.to(device)\n",
    "        print (seq.size())\n",
    "        # Forward prop.\n",
    "        out = encoder(seq, attn_masks)\n",
    "        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(out, ans, ans_len)\n",
    "\n",
    "        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
    "        #print (caps_sorted.size())\n",
    "        \n",
    "        targets = (caps_sorted[:, 1:].clone())\n",
    "        #print (targets)\n",
    "        \n",
    "        if (epoch%1==0):\n",
    "            with torch.no_grad():\n",
    "                ans=[]\n",
    "                for t in range(max(decode_lengths)):\n",
    "                    #print (scores.size())\n",
    "                    data = scores[0,t,:]\n",
    "                    data = F.softmax(data, dim=0)\n",
    "                    #print (data.size())\n",
    "\n",
    "                    val = (max(data))\n",
    "                    for j in range(30522):\n",
    "                        if data[j]==val:\n",
    "                            predicted_token = tokenizer.convert_ids_to_tokens([j])[0]\n",
    "                            ans.append(predicted_token)\n",
    "                            break\n",
    "                Ans=[]\n",
    "                #print (scores.view(-1,30522).size())\n",
    "                #print (targets.squeeze(1).size())\n",
    "                for j in (targets[0]):\n",
    "                    #print (j.data.tolist())\n",
    "                    Ans.append(tokenizer.convert_ids_to_tokens([j.data.tolist()]))\n",
    "                print (ans)\n",
    "                print (Ans)\n",
    "        # Remove timesteps that we didn't decode at, or are pads\n",
    "        #scores, _ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
    "        #targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print (scores.size())\n",
    "        #print (targets.size())\n",
    "        t = targets\n",
    "        #print (t.view(-1,1).size())\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(scores.view(-1,30522), targets.view(-1,1).squeeze(1))\n",
    "\n",
    "        # Add doubly stochastic attention regularization\n",
    "        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "        print (loss.item())\n",
    "        # Back prop.\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        \n",
    "        # Update weights\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        \n",
    "\n",
    "def main():\n",
    "    global epochs_since_improvement, checkpoint, start_epoch, data_name, word_map\n",
    "    \n",
    "    \n",
    "    decoder = DecoderWithAttention(embed_dim=emb_dim,\n",
    "                                   decoder_dim=decoder_dim,\n",
    "                                   vocab_size=30522,\n",
    "                                   dropout=dropout)\n",
    "    decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
    "                                         lr=decoder_lr)\n",
    "    \n",
    "    encoder = Encoder(256,freeze_bert = True)\n",
    "    encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
    "                                         lr=decoder_lr)\n",
    "    # Move to GPU, if available\n",
    "    decoder = decoder.to(device)\n",
    "    encoder = encoder.to(device)\n",
    "    if checkpoint is not None:\n",
    "        checkpoint = torch.load(checkpoint)\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
    "        decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "        \n",
    "        decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer_dict'])\n",
    "        encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "        encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer_dict'])\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    dataloader = DataLoader(train_set, batch_size = 16, num_workers = 0)\n",
    "    start = time.time()\n",
    "    while(1):\n",
    "        for epoch in range(start_epoch, epochs):\n",
    "\n",
    "\n",
    "            current_time = time.time()\n",
    "            if (current_time-start >= 6*3600):\n",
    "                print (\"2 minutes completed \")\n",
    "                print (epoch)\n",
    "                #save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, decoder_optimizer)\n",
    "                break\n",
    "            train(dataloader, encoder, decoder, criterion,encoder_optimizer, decoder_optimizer, epoch ,start)\n",
    "\n",
    "        print (\"EPOCHS COMPLETED\")\n",
    "        save_checkpoint(data_name, 0, epochs_since_improvement, encoder, decoder,encoder_optimizer, decoder_optimizer)\n",
    "        if (current_time-start >= 6*3600):\n",
    "            print (\"2 minutes completed \")\n",
    "            print (epoch)\n",
    "            #save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, decoder_optimizer)\n",
    "            break\n",
    "#main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EVALUATING ##\n",
    "\n",
    "\n",
    "decoder = DecoderWithAttention(embed_dim=emb_dim,\n",
    "                                   decoder_dim=decoder_dim,\n",
    "                                   vocab_size=30522,\n",
    "                                   dropout=dropout)\n",
    "\n",
    "encoder = Encoder(256,freeze_bert = True)\n",
    "decoder = decoder.to(device)\n",
    "encoder = encoder.to(device)\n",
    "checkpoint = '../input/chatbot/checkpoint1.pt'\n",
    "if checkpoint is not None:\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chemistry\n",
      "['my', 'favorite', 'subject', 'is', 'chemistry', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "encoder = encoder.eval()\n",
    "decoder = decoder.eval()\n",
    "\n",
    "question = \"chemistry\"\n",
    "maxlen = 15\n",
    "tokens = tokenizer.tokenize(question) #Tokenize the sentence\n",
    "tokens = ['[CLS]'] + tokens + ['[SEP]'] #Insering the CLS and SEP token in the beginning and end of the sentence\n",
    "if len(tokens) < maxlen:\n",
    "    tokens = tokens + ['[PAD]' for _ in range(maxlen - len(tokens))] #Padding sentences\n",
    "else:\n",
    "    tokens = tokens[:maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n",
    "#print (tokens)\n",
    "tokens_ids = tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
    "#print (tokens_ids)\n",
    "tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor\n",
    "\n",
    "#Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
    "attn_mask = (tokens_ids_tensor != 0).long().unsqueeze(0)\n",
    "seq = tokens_ids_tensor.unsqueeze(0)\n",
    "\n",
    "# Forward prop.\n",
    "out = encoder(seq, attn_mask)\n",
    "pred = torch.LongTensor([[tokenizer.convert_tokens_to_ids('[CLS]')]]).to(device)\n",
    "h,c = decoder.init_hidden_state(out) \n",
    "\n",
    "sampled = []\n",
    "for t in range(20):\n",
    "    embeddings = decoder.embedding(pred).squeeze(1)  \n",
    "    attention_weighted_encoding, alpha = decoder.attention(out,h)\n",
    "    gate = decoder.sigmoid(decoder.f_beta(h))  \n",
    "    attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "    h, c = decoder.decode_step(\n",
    "        torch.cat([embeddings, attention_weighted_encoding], dim=1),(h, c))  # (batch_size_t, decoder_dim)\n",
    "    pt = decoder.fc(decoder.dropout(h))  # (batch_size_t, vocab_size)\n",
    "    _,pred = pt.max(1)\n",
    "    sampled.append(pred.item())\n",
    "\n",
    "#print (sampled)\n",
    "print (question)\n",
    "answer = []\n",
    "for i in sampled:\n",
    "    answer.append(tokenizer.convert_ids_to_tokens(i))\n",
    "    if (answer[-1]=='[SEP]'):\n",
    "        break\n",
    "\n",
    "print (answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14, 10])\n",
      "torch.Size([140, 1])\n",
      "torch.Size([14])\n",
      "tensor(2.3712)\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(0,5)\n",
    "torch.nn.functional.one_hot(x)\n",
    "from torch.autograd import Variable\n",
    "output = (torch.rand(14,10))\n",
    "target = (torch.LongTensor(14))\n",
    "for i in range(14):\n",
    "    target[i] = i%10\n",
    "print (output.size())\n",
    "print (output.view(-1,1).size())\n",
    "print (target.size())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0a278d40dad946fea58bf64941444c2a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "124cceaee3b84b4898244fa997aff3f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0a278d40dad946fea58bf64941444c2a",
       "max": 440473133,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_24903d737485486eb2c6fa707948c7bb",
       "value": 440473133
      }
     },
     "228a8740cc844d1d9f7593e88730d1cd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "24903d737485486eb2c6fa707948c7bb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "47c4846edd244de19a8ab6396abf2612": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9c63ff72aa9a43d28d43d0255f09e8f2",
        "IPY_MODEL_c02cc88c43714f24bc845196ca741b25"
       ],
       "layout": "IPY_MODEL_7b673b02f0104a44ab0a4d361d8fa63b"
      }
     },
     "498d8bec016e4be2a14032251c15a640": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "501cfc1162154e1094f2cf458ae7bc1f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "54d824e669ae4c18b7737dd9357c86f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_902a99c6e6b8419cb93c219824d136fc",
        "IPY_MODEL_83ef23f0eb4e447989d188858b91c65d"
       ],
       "layout": "IPY_MODEL_a72c0022dc634eefb782b83e9def5e1d"
      }
     },
     "57666af286d248a1a8697c6ed011af73": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "6da5f74cbb3a428ea01729051c863481": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6f3e6ce620164d02b0a5ebeeae306919": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "733204a9091b4acaa4550a70e3f24607": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "78f838819def4bf8b3b7e53bba86129e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_124cceaee3b84b4898244fa997aff3f2",
        "IPY_MODEL_e44d27e4da8c49eba5fa6d7e678b1207"
       ],
       "layout": "IPY_MODEL_501cfc1162154e1094f2cf458ae7bc1f"
      }
     },
     "7b673b02f0104a44ab0a4d361d8fa63b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "83ef23f0eb4e447989d188858b91c65d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_733204a9091b4acaa4550a70e3f24607",
       "placeholder": "​",
       "style": "IPY_MODEL_b52a66df2c6c49738ff4dba50ea28090",
       "value": " 232k/232k [00:00&lt;00:00, 294kB/s]"
      }
     },
     "902a99c6e6b8419cb93c219824d136fc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b1671e745f354ec88fe7a8bfb0c5aa12",
       "max": 231508,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_228a8740cc844d1d9f7593e88730d1cd",
       "value": 231508
      }
     },
     "972fa522d5ac4022a61a4e618ad357f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9c63ff72aa9a43d28d43d0255f09e8f2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "Downloading: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6da5f74cbb3a428ea01729051c863481",
       "max": 361,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_57666af286d248a1a8697c6ed011af73",
       "value": 361
      }
     },
     "a72c0022dc634eefb782b83e9def5e1d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b1671e745f354ec88fe7a8bfb0c5aa12": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b52a66df2c6c49738ff4dba50ea28090": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "c02cc88c43714f24bc845196ca741b25": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_6f3e6ce620164d02b0a5ebeeae306919",
       "placeholder": "​",
       "style": "IPY_MODEL_972fa522d5ac4022a61a4e618ad357f8",
       "value": " 361/361 [00:01&lt;00:00, 263B/s]"
      }
     },
     "e44d27e4da8c49eba5fa6d7e678b1207": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e8676132eb294339bc885aa5d547e41b",
       "placeholder": "​",
       "style": "IPY_MODEL_498d8bec016e4be2a14032251c15a640",
       "value": " 440M/440M [00:40&lt;00:00, 11.0MB/s]"
      }
     },
     "e8676132eb294339bc885aa5d547e41b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
